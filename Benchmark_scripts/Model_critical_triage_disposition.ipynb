{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling for predicting critical outcome at ED disposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work Flow:\n",
    "1. Task-specific filter\n",
    "2. Variable selection\n",
    "3. Modeling script\n",
    "4. Performance output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jun/data/MIMIC/mimiciv/processed/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2165424/3785985864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#output_path = '/Users/siqili/Desktop/National University of Singapore/Liu Nan - WP_MIMIC_Benchmark/Figure3/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C:/Users/XFE/Documents/mimic4ed-benchmark/data_processed/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mconfidence_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jun/data/MIMIC/mimiciv/processed/train.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "from helpers import PlotROCCurve, get_lstm_data_gen\n",
    "\n",
    "#path = 'C:/Users/XFE/Documents/mimic4ed-benchmark/data_processed/'\n",
    "#path = 'C:/Users/XFE/Documents/mimic4ed-benchmark/data_processed/'\n",
    "path = '/home/jun/data/MIMIC/processed'\n",
    "#path = '/Users/siqili/Desktop/National University of Singapore/Liu Nan - WP_MIMIC_Benchmark'\n",
    "#output_path = '/Users/siqili/Desktop/National University of Singapore/Liu Nan - WP_MIMIC_Benchmark/Figure3/'\n",
    "output_path = 'C:/Users/XFE/Documents/mimic4ed-benchmark/data_processed/'\n",
    "df_train = pd.read_csv((os.path.join(path, 'train.csv')))\n",
    "df_test = pd.read_csv((os.path.join(path, 'test.csv')))\n",
    "confidence_interval = 95\n",
    "random_seed = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100) \n",
    "pd.set_option('display.max_rows', 100) \n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. task-specific filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train['before_ed_mortality'] == False) & (df_train['ed_death'] == False)]\n",
    "df_test = df_test[(df_test['before_ed_mortality'] == False) & (df_test['ed_death'] == False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = [\"age\", \"gender\", \n",
    "            \n",
    "            \"n_ed_30d\", \"n_ed_90d\", \"n_ed_365d\", \"n_hosp_30d\", \"n_hosp_90d\", \n",
    "            \"n_hosp_365d\", \"n_icu_30d\", \"n_icu_90d\", \"n_icu_365d\", \n",
    "            \n",
    "            \"triage_pain\", \"triage_acuity\",\n",
    "            \n",
    "            \"chiefcom_chest_pain\", \"chiefcom_abdominal_pain\", \"chiefcom_headache\", \n",
    "            \"chiefcom_shortness_of_breath\", \"chiefcom_back_pain\", \"chiefcom_cough\", \n",
    "            \"chiefcom_nausea_vomiting\", \"chiefcom_fever_chills\", \"chiefcom_syncope\",\n",
    "            \"chiefcom_dizziness\",\n",
    "            \n",
    "            \"cci_MI\", \"cci_CHF\", \"cci_PVD\", \"cci_Stroke\", \"cci_Dementia\", \"cci_Pulmonary\", \n",
    "            \"cci_Rheumatic\", \"cci_PUD\", \"cci_Liver1\", \"cci_DM1\", \"cci_DM2\", \n",
    "            \"cci_Paralysis\", \"cci_Renal\", \"cci_Cancer1\", \"cci_Liver2\", \"cci_Cancer2\", \n",
    "            \"cci_HIV\",\n",
    "            \n",
    "            \"eci_Arrhythmia\", \"eci_Valvular\", \"eci_PHTN\",  \"eci_HTN1\", \"eci_HTN2\",  \n",
    "            \"eci_NeuroOther\", \"eci_Hypothyroid\", \"eci_Lymphoma\", \"eci_Coagulopathy\", \n",
    "            \"eci_Obesity\", \"eci_WeightLoss\", \"eci_FluidsLytes\", \"eci_BloodLoss\", \n",
    "            \"eci_Anemia\", \"eci_Alcohol\", \"eci_Drugs\", \"eci_Psychoses\", \"eci_Depression\",\n",
    "            \n",
    "            \"ed_temperature_last\", \"ed_heartrate_last\", \"ed_resprate_last\", \n",
    "            \"ed_o2sat_last\", \"ed_sbp_last\", \"ed_dbp_last\", \"ed_los\", \"n_med\", \"n_medrecon\"]\n",
    "\n",
    "outcome = \"outcome_critical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[variable].copy()\n",
    "y_train = df_train[outcome].copy()\n",
    "X_test = df_test[variable].copy()\n",
    "y_test = df_test[outcome].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "X_train['gender'] = encoder.fit_transform(X_train['gender'])\n",
    "X_test['gender'] = encoder.transform(X_test['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ed_los'] = pd.to_timedelta(X_train['ed_los']).dt.seconds / 60\n",
    "X_test['ed_los'] = pd.to_timedelta(X_test['ed_los']).dt.seconds / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class ratio')\n",
    "print('positiave : negative =', y_train.sum()/(~y_train).sum(), ': 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Modeling script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers for all results\n",
    "result_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(random_state=random_seed)\n",
    "start = time.time()\n",
    "logreg.fit(X_train,y_train)\n",
    "runtime = time.time()-start\n",
    "print('Training time:', runtime, 'seconds')\n",
    "probs = logreg.predict_proba(X_test)\n",
    "result = PlotROCCurve(probs[:,1],y_test, ci=confidence_interval, random_seed=random_seed)\n",
    "\n",
    "results = [\"LR\"]\n",
    "results.extend(result)\n",
    "results.append(runtime)\n",
    "result_list.append(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RandomForest:\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf= RandomForestClassifier(random_state=random_seed)\n",
    "start = time.time()\n",
    "rf.fit(X_train,y_train)\n",
    "runtime = time.time()-start\n",
    "print('Training time:', runtime, 'seconds')\n",
    "probs = rf.predict_proba(X_test)\n",
    "result = PlotROCCurve(probs[:,1],y_test, ci=confidence_interval, random_seed=random_seed)\n",
    "importances = rf.feature_importances_\n",
    "print(importances)\n",
    "\n",
    "results = [\"RF\"]\n",
    "results.extend(result)\n",
    "results.append(runtime)\n",
    "result_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GradientBoosting:\")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(random_state=random_seed)\n",
    "start = time.time()\n",
    "gb.fit(X_train, y_train)\n",
    "runtime = time.time()-start\n",
    "print('Training time:', runtime, 'seconds')\n",
    "probs = gb.predict_proba(X_test)\n",
    "result = PlotROCCurve(probs[:,1],y_test, ci=confidence_interval, random_seed=random_seed)\n",
    "\n",
    "results = [\"GB\"]\n",
    "results.extend(result)\n",
    "results.append(runtime)\n",
    "result_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal skip\n",
    "'''\n",
    "print(\"Support Vector Machine:\")\n",
    "from sklearn.svm import SVC\n",
    "svc=SVC()\n",
    "start = time.time()\n",
    "svc.fit(X_train,y_train)\n",
    "runtime = time.time()-start\n",
    "print('Training time:', runtime, 'seconds')\n",
    "probs = svc.predict_proba(X_test)\n",
    "result = PlotROCCurve(probs[:,1],y_test, ci=confidence_interval)\n",
    "\n",
    "results = [\"SVM\"]\n",
    "results.extend(result)\n",
    "results.append(runtime)\n",
    "result_list.append(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense_1 = Dense(128, activation='relu')\n",
    "        self.dense_2 = Dense(64, activation='relu')\n",
    "        self.classifier = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy', 'AUC', {'auprc': metrics.AUC(name='auprc', curve='PR')}, \n",
    "                       'TruePositives', 'TrueNegatives', 'Precision', 'Recall'])\n",
    "start = time.time()\n",
    "mlp.fit(X_train.astype(np.float32), y_train, batch_size=200, epochs=20)\n",
    "runtime = time.time()-start\n",
    "print('Training time:', runtime, 'seconds')\n",
    "mlp.save('critical_disposition_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP:\")\n",
    "mlp = load_model('critical_disposition_mlp')\n",
    "probs = mlp.predict(X_test.astype(np.float32))\n",
    "result = PlotROCCurve(probs,y_test)\n",
    "results = [\"MLP\"]\n",
    "results.extend(result)\n",
    "results.append(runtime)\n",
    "result_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data for LSTM\n",
    "resample_freq = '1H'\n",
    "df_vitalsign = pd.read_csv('ed_vitalsign_' + resample_freq + '_resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen, test_data_gen = get_lstm_data_gen(df_train, df_test, df_vitalsign, variable, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LSTM_MLP, self).__init__()\n",
    "        self.dense_1 = Dense(96, activation='relu')\n",
    "        self.lstm = LSTM(32)\n",
    "        self.dense_2 = Dense(64, activation='relu')\n",
    "        self.classifier = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x1, x2 = x\n",
    "        x = self.dense_1(x1)\n",
    "        lstm_output = self.lstm(x2)\n",
    "        x = concatenate([x, lstm_output])\n",
    "        x = self.dense_2(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM_MLP()\n",
    "lstm.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy', 'AUC', {'auprc': metrics.AUC(name='auprc', curve='PR')}, \n",
    "                       'TruePositives', 'TrueNegatives', 'Precision', 'Recall'])\n",
    "\n",
    "start = time.time()\n",
    "lstm.fit(train_data_gen, batch_size=200, epochs=20)\n",
    "runtime = time.time()-start\n",
    "print('Training time:', runtime, 'seconds')\n",
    "lstm.save('critial_disposition_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM:\")\n",
    "lstm = load_model('critial_disposition_lstm')\n",
    "probs = lstm.predict(test_data_gen)\n",
    "result = PlotROCCurve(probs,y_test)\n",
    "results = [\"LSTM\"]\n",
    "results.extend(result)\n",
    "results.append(runtime)\n",
    "result_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_performance(s, random_seed=0):\n",
    "    print(s)\n",
    "    score = np.array(df_test[s])\n",
    "    result = PlotROCCurve(score,y_test, ci=confidence_interval, random_seed=random_seed)\n",
    "    runtime = 0\n",
    "    results = [s]\n",
    "    results.extend(result)\n",
    "    results.append(runtime)\n",
    "    result_list.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"ESI\"] = -df_test[\"triage_acuity\"]\n",
    "get_score_performance(\"ESI\", random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_score_performance(\"score_NEWS\", random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score_performance(\"score_NEWS2\", random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score_performance(\"score_REMS\", random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score_performance(\"score_MEWS\", random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score_performance(\"score_CART\", random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df = pd.read_csv(os.path.join(path, 'result_critical_triage.csv'))\n",
    "result_df = pd.DataFrame(result_list, columns=['Model', 'auroc', 'ap', 'sensitivity', 'specificity', 'Threshold', \n",
    "                                               'lower_auroc', 'upper_auroc', 'std_auroc', 'lower_ap', 'upper_ap', \n",
    "                                               'std_ap', 'lower_sensitivity', 'upper_sensitivity', 'std_sensitivity', \n",
    "                                               'lower_specificity', 'upper_specificity', 'std_specificity', 'Runtime'])\n",
    "result_df = result_df.round(3)\n",
    "result_df['AUROC'] = result_df['auroc'].astype(str) + ' (' + result_df['lower_auroc'].astype(str) + \\\n",
    "                     '-' + result_df['upper_auroc'].astype(str) + ')'\n",
    "result_df['AUPRC'] = result_df['ap'].astype(str) + ' (' + result_df['lower_ap'].astype(str) + \\\n",
    "                     '-' + result_df['upper_ap'].astype(str) + ')'\n",
    "result_df['Sensitivity'] = result_df['sensitivity'].astype(str) + ' (' + result_df['lower_sensitivity'].astype(str) + \\\n",
    "                           '-' + result_df['upper_sensitivity'].astype(str) + ')'\n",
    "result_df['Specificity'] = result_df['specificity'].astype(str) + ' (' + result_df['lower_specificity'].astype(str) + \\\n",
    "                           '-' + result_df['upper_specificity'].astype(str) + ')'\n",
    "result_df = result_df[['Model', 'Threshold', 'AUROC', 'AUPRC', 'Sensitivity', 'Specificity', 'Runtime']]\n",
    "result_df.to_csv(os.path.join(path, 'result_critical_disposition.csv'), index=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result_list)[[0, 1, 2,3, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_confidence_interval\n",
    "plot_confidence_interval(result_df, metric='auroc', ci=confidence_interval, name = \"AUROC\", \n",
    "                         my_file = 'AUROC_critical.eps', my_path = output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_interval(result_df, metric='ap', ci=confidence_interval, name = \"AUPRC\", \n",
    "                         my_file = 'AUPRC_critical.eps', my_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(zip(variable, importances), columns=['Variable','Importance'])\n",
    "# importance_df.to_csv(os.path.join(path, 'importances_critical_triage.csv'))\n",
    "importance_df.sort_values(by='Importance', axis=0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf9f7c42999d1346969e890247ad220cd768e54a9022ee81cbe62d07cccdebca"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
